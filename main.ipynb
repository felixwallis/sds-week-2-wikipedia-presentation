{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from modules.wiki_parser import FileProcessor, WikiTextExtractor\n",
    "from modules.nlp import NLP\n",
    "\n",
    "DATA_DIR = Path().cwd() / \"data\"\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 687/2001 [00:10<00:18, 72.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/felixwallis/Desktop/Oxford MSc/Oxford Social Data Science Course/Fundamentals for Social Data Science in Python/sds-week-2-wikipedia-presentation/data/articles/Vladimir_Putin/2023/05/1155008451.xml: too many values to unpack (expected 2)\n",
      "Error processing /Users/felixwallis/Desktop/Oxford MSc/Oxford Social Data Science Course/Fundamentals for Social Data Science in Python/sds-week-2-wikipedia-presentation/data/articles/Vladimir_Putin/2023/05/1155008376.xml: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1019/2001 [00:15<00:12, 77.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/felixwallis/Desktop/Oxford MSc/Oxford Social Data Science Course/Fundamentals for Social Data Science in Python/sds-week-2-wikipedia-presentation/data/articles/Xi_Jinping/2022/11/1124670506.xml: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1568/2001 [00:22<00:04, 89.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/felixwallis/Desktop/Oxford MSc/Oxford Social Data Science Course/Fundamentals for Social Data Science in Python/sds-week-2-wikipedia-presentation/data/articles/Xi_Jinping/2024/08/1242366887.xml: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1803/2001 [00:25<00:02, 77.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/felixwallis/Desktop/Oxford MSc/Oxford Social Data Science Course/Fundamentals for Social Data Science in Python/sds-week-2-wikipedia-presentation/data/articles/Xi_Jinping/2023/.DS_Store: 'utf-8' codec can't decode byte 0xb8 in position 95: invalid start byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:27<00:00, 72.07it/s]\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_DIR = DATA_DIR / \"articles\"\n",
    "\n",
    "file_processor = FileProcessor()\n",
    "wiki_text_extractor = WikiTextExtractor()\n",
    "\n",
    "# Fetch all article file paths\n",
    "file_paths = file_processor.fetch_file_paths(ARTICLE_DIR)\n",
    "\n",
    "# Extract text from all articles\n",
    "article_texts = [wiki_text_extractor.process_file(file_path) for file_path in tqdm(file_paths)]\n",
    "articles_df = pd.concat(article_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(\n",
    "    DATA_DIR / \"glove.840B.300d.txt\", binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing article text: 100%|██████████| 98832/98832 [00:51<00:00, 1932.76it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = NLP()\n",
    "\n",
    "# Tokenize text and remove stop words\n",
    "tqdm.pandas(desc=\"Tokenizing article text\")\n",
    "articles_df['tokens'] = articles_df['section_text'].progress_apply(nlp.tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating weighted embeddings: 100%|██████████| 98832/98832 [02:13<00:00, 742.34it/s]\n"
     ]
    }
   ],
   "source": [
    "articles_df['weighted_embeddings'] = nlp.generate_tfidf_weighted_embeddings(articles_df['tokens'], word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the number of revisions over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
